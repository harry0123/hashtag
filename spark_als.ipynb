{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "a = open('handle_to_id.p', 'rb')\n",
      "b = open('sn_to_userid.p', 'rb')\n",
      "c = open('total_set.p', 'rb')\n",
      "handle_to_id = pickle.load(a)\n",
      "sn_to_id = pickle.load(b)\n",
      "total_set = pickle.load(c)\n",
      "a.close()\n",
      "b.close()\n",
      "c.close()\n",
      "\n",
      "print len(handle_to_id)\n",
      "print len(sn_to_id)\n",
      "print len(total_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "194\n",
        "14808\n",
        "90613\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "path_to_spark = '/home/ubuntu/datascience-sp14/hw3/spark-0.9.1-bin-cdh4'\n",
      "os.environ['SPARK_HOME'] = path_to_spark\n",
      "\n",
      "# Set the python path so that we know where to find the pyspark files.\n",
      "import sys\n",
      "path_to_pyspark = os.path.join(path_to_spark, \"python\")\n",
      "sys.path.insert(0, path_to_pyspark)\n",
      "\n",
      "from pyspark import SparkContext\n",
      "# You can set the app name to whatever you want; this just affects what\n",
      "# will show up in the UI.\n",
      "app_name = \"i<3datascience\"\n",
      "sc = SparkContext(\"local\", app_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#reindex_sn = {}\n",
      "#reindex_handle = {}\n",
      "#reindexed = []\n",
      "#sn_counter = 1\n",
      "#handle_counter = 1\n",
      "#for tweet in total_set:\n",
      "#    if tweet[0] in reindex_sn:\n",
      "#        sn_index = reindex_sn[tweet[0]]\n",
      "#    else:\n",
      "#        sn_index = sn_counter\n",
      "#        reindex_sn[tweet[0]] = sn_index\n",
      "#        sn_counter = sn_counter + 1\n",
      "#    if tweet[1] in reindex_handle:\n",
      "#        handle_index = reindex_handle[tweet[1]]\n",
      "#    else:\n",
      "#        handle_index = handle_counter\n",
      "#        reindex_handle[tweet[1]] = handle_index\n",
      "#        handle_counter = handle_counter + 1\n",
      "#    val = int( tweet[2] == 1) #* 4 + 1\n",
      "#    \n",
      "#    reindexed.append((sn_index, handle_index, val))\n",
      "#all_data = sc.parallelize(reindexed)\n",
      "#\n",
      "#print sn_counter\n",
      "#print handle_counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "all_data = sc.parallelize(total_set)\n",
      "all_data = all_data.map(lambda x: (x[0], x[1],x[2], random.randint(0,9))).cache()\n",
      "all_data.count()\n",
      "\n",
      "all_data.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 123,
       "text": [
        "[(5939, 110, 0, 7),\n",
        " (9762, 104, 0, 2),\n",
        " (11063, 134, 0, 3),\n",
        " (3993, 186, 0, 8),\n",
        " (2334, 7, 0, 5),\n",
        " (3279, 97, 0, 3),\n",
        " (2130, 55, 0, 6),\n",
        " (7703, 94, 0, 6),\n",
        " (10584, 58, 0, 3),\n",
        " (11923, 163, 0, 7)]"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "\n",
      "#all_data = sc.parallelize(total_set)\n",
      "training = all_data.filter(lambda x: x[3] in range(0,5))\n",
      "validation = all_data.filter(lambda x: x[3] in range(6,8))\n",
      "test = all_data.filter(lambda x: x[3] in range(8,10))\n",
      "\n",
      "print all_data.count()\n",
      "print training.count()\n",
      "print validation.count()\n",
      "print test.count()\n",
      "#print \"Training: %s, validation: %s, test: %s\" % (training.count(), validation.count(), test.count())\n",
      "print training.take(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "90613\n",
        "45641"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17917"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17998"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(9762, 104, 0, 2), (11063, 134, 0, 3), (3279, 97, 0, 3), (10584, 58, 0, 3), (9318, 138, 0, 3), (4427, 62, 0, 0), (673, 94, 0, 3), (6218, 191, 0, 4), (6635, 174, 0, 4), (13433, 173, 0, 4), (1993, 43, 0, 2), (1046, 33, 0, 0), (5309, 58, 0, 0), (3334, 90, 0, 0), (11860, 74, 0, 2), (10284, 189, 0, 2), (5638, 186, 0, 4), (8014, 77, 0, 4), (3211, 4, 0, 4), (8328, 50, 0, 0), (4514, 30, 0, 4), (13298, 120, 0, 4), (14007, 134, 0, 2), (4588, 126, 0, 3), (5200, 102, 0, 0), (11354, 78, 0, 0), (4276, 173, 0, 4), (4091, 41, 0, 1), (3228, 34, 0, 3), (10463, 147, 0, 3), (10615, 62, 0, 1), (1653, 141, 0, 3), (6336, 110, 0, 3), (3105, 192, 0, 4), (9688, 30, 0, 3), (1501, 94, 0, 2), (5112, 74, 0, 3), (14233, 139, 1, 0), (12662, 95, 0, 4), (1938, 50, 0, 3), (2536, 138, 0, 2), (11220, 192, 0, 0), (3390, 84, 1, 1), (13290, 21, 0, 0), (2069, 129, 0, 0), (5884, 28, 0, 1), (6012, 7, 0, 2), (5735, 117, 0, 0), (9575, 185, 0, 0), (4363, 94, 0, 2)]\n"
       ]
      }
     ],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "def compute_error(predicted, actual):\n",
      "    \"\"\" Compute the root mean squared error between predicted and actual.\n",
      "    \n",
      "    Params:\n",
      "      predicted: An RDD of predicted ratings for each movie and each user where each entry is in the form (user, movie, rating).\n",
      "      actual: An RDD of actual ratings where each entry is in the form (user, movie, rating).\n",
      "    \"\"\"\n",
      "    # Make each RDD in the format ((user, movie), rating) so we can easily join them together.\n",
      "    predicted_reformatted = predicted.map(lambda x: ((x[0], x[1]), x[2]))\n",
      "    actual_reformatted = actual.map(lambda x: ((x[0], x[1]), x[2]))\n",
      "    predicted_and_actual = predicted_reformatted.join(actual_reformatted)\n",
      "    squared_errors = predicted_and_actual.map(lambda x: (x[1][1] - x[1][0])**2)\n",
      "    total_error = squared_errors.reduce(lambda x,y: x + y)\n",
      "    num_ratings = squared_errors.count()\n",
      "    return math.sqrt(total_error * 1.0 / num_ratings)\n",
      "\n",
      "test_predicted = sc.parallelize([\n",
      "    (1, 1, 5),\n",
      "    (1, 2, 3),\n",
      "    (1, 3, 4),\n",
      "    (2, 1, 3),\n",
      "    (2, 2, 2),\n",
      "    (2, 3, 4)])\n",
      "test_actual = sc.parallelize([\n",
      "     (1, 2, 3),\n",
      "     (1, 3, 5),\n",
      "     (2, 1, 5),\n",
      "     (2, 2, 1)])\n",
      "print \"Error for test datasets: %s\" % compute_error(test_predicted, test_actual)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Error for test datasets: 1.22474487139\n"
       ]
      }
     ],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.recommendation import ALS\n",
      "\n",
      "formatted_training = training.map(lambda x: (x[0], x[1], x[2])).cache()\n",
      "training2 = training.map(lambda x: (x[0], x[1]))\n",
      "validation_for_predict = validation.map(lambda x: (x[0], x[1])).cache()\n",
      "validation_for_error = validation.map(lambda x: (x[0], x[1], x[2]))\n",
      "\n",
      "print validation_for_predict.count()\n",
      "print validation_for_error.count()\n",
      "\n",
      "\n",
      "ranks = [1, 2, 4, 8, 12, 16]\n",
      "\n",
      "min_error = float(\"inf\")\n",
      "best_rank = -1\n",
      "for rank in ranks:\n",
      "    model = ALS.train(formatted_training, rank)\n",
      "    predicted_ratings = model.predictAll(validation_for_predict)\n",
      "    error = compute_error(predicted_ratings, validation_for_error)\n",
      "    print rank, error\n",
      "    if error < min_error:\n",
      "        min_error = error\n",
      "        best_rank = rank\n",
      "\n",
      "print \"The best model was trained with rank %s\" % best_rank\n",
      "\n",
      "test_for_predicting = test.map(lambda x: (x[0], x[1]))\n",
      "model = ALS.train(formatted_training, best_rank)\n",
      "predicted_test = model.predictAll(test_for_predicting)\n",
      "test_for_error = test.map(lambda x: (x[0], x[1], x[2]))\n",
      "\n",
      "test_rmse = compute_error(test_for_error, predicted_test)\n",
      "\n",
      "print \"The model had a RMSE on the test set of %s\" % test_rmse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "17917\n",
        "17917"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.187644806958\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.208628206037\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.211964770087\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.233519862476\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.244940032285\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.257860616273\n",
        "The best model was trained with rank 1\n",
        "The model had a RMSE on the test set of 0.190777898941"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}