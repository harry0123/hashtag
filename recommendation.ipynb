{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import string\n",
      "import json\n",
      "import nltk\n",
      "#DATA_PATH = \"/home/ubuntu/datascience-sp14/final/hashtag/\"\n",
      "DATA_PATH = \"/home/saasbook/hashtag/\"\n",
      "tweets = pd.read_csv(DATA_PATH+'tweets.csv', low_memory=False).drop_duplicates()\n",
      "\n",
      "#print tweets.info()\n",
      "tweets = tweets[['id_str', 'screen_name', 'created_at', 'text', 'retweet_count', 'hashtags', 'lat', 'lng']]\n",
      "print tweets.info()\n",
      "handles = json.load(open(DATA_PATH+'handle2name.json')).keys()\n",
      "handles = set([str(handle) for handle in handles])\n",
      "mentions = [sum([handle in text for handle in handles]) for text in tweets['text']]\n",
      "tweets['artist mentions'] = mentions\n",
      "filteredtweets = tweets[tweets['artist mentions'] != 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 206672 entries, 0 to 206671\n",
        "Data columns (total 8 columns):\n",
        "id_str           206672  non-null values\n",
        "screen_name      206672  non-null values\n",
        "created_at       206672  non-null values\n",
        "text             206672  non-null values\n",
        "retweet_count    206672  non-null values\n",
        "hashtags         84666  non-null values\n",
        "lat              7012  non-null values\n",
        "lng              6273  non-null values\n",
        "dtypes: float64(1), int64(2), object(5)None\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stopwords = [line.rstrip() for line in open(DATA_PATH + \"stopwords.txt\", 'r')] # Load from file\n",
      "quickbrownfox = \"A quick brown fox jumps over the lazy dog\"\n",
      "import re\n",
      "\n",
      "def simple_tokenize(string):\n",
      "    split_regex = r'[\\s\\xe2\\x80\\x9c\\xe2\\x80\\x9d]+'\n",
      "    return filter(None, re.split(split_regex, string.lower()))\n",
      "\n",
      "\n",
      "def tokenize(string):\n",
      "    tmp = [word for word in simple_tokenize(string) if word not in stopwords]\n",
      "    tokens = []\n",
      "    for word in tmp:\n",
      "        if \"http://\" not in word:\n",
      "            tok = \"\".join(l for l in word if l not in ['!','.',':','?','\\\\','$','&','%','\\\"','(',')','*',',',';','_','`','~','+'])\n",
      "            if len(tok) > 0:\n",
      "                tokens.append(tok)\n",
      "    return tokens\n",
      "\n",
      "#print tokenize(quickbrownfox) # Should give ['quick', 'brown', ... ]\n",
      "\n",
      "print tweets[tweets['id_str']==454479340789837824].text.item()\n",
      "print type(tweets[tweets['id_str']==454479340789837824].text.item())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u201c@StyleList: An outfit for each day of #Coachella's first weekend: http://t.co/5xGyX7QfH3 http://t.co/ZOZZc8GF5F\u201d\n",
        "<type 'str'>\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweets['text_tokens'] = tweets['text'].apply(tokenize)\n",
      "print tweets[tweets['id_str']==454479340789837824].text_tokens.item()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['@stylelist', 'outfit', 'day', \"#coachella's\", 'first', 'weekend']\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "train = []\n",
      "count = 0\n",
      "with open('clean_sample.csv', 'rb') as csvfile:\n",
      "    samplereader = csv.reader(csvfile, delimiter=',')\n",
      "    for row in samplereader:\n",
      "        #print row\n",
      "        if count >1000:\n",
      "            break\n",
      "        if len(row) > 1:\n",
      "            train.append((row[0], row[1]))\n",
      "            count += 1\n",
      "        #print row\n",
      "        #break\n",
      "        \n",
      "#print train\n",
      "#print len(train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def feature_selection(string):\n",
      "    tmp = [word for word in simple_tokenize(string) if word not in stopwords]\n",
      "    tokens = []\n",
      "    for word in tmp:\n",
      "        if \"@\" in word:\n",
      "            continue\n",
      "        if len(word) < 3:\n",
      "            continue\n",
      "        if \"http://\" not in word:\n",
      "            tok = \"\".join(l for l in word if l not in ['!','.',':','?','\\\\','$','&','%','\\\"','(',')','*',',',';','_','`','~','+'])\n",
      "            if len(tok) > 0:\n",
      "                tokens.append(tok)\n",
      "    return tokens\n",
      "\n",
      "all_words = []\n",
      "for text in filteredtweets['text']:\n",
      "    for word in feature_selection(text):\n",
      "        all_words.append(word)\n",
      "print len(all_words)\n",
      "\n",
      "fdist = nltk.FreqDist(all_words)\n",
      "common_words = fdist.keys()[:1000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "132014\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = [({word: (word in feature_selection(x[0])) for word in common_words}, x[1]) for x in train]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "classifier = nltk.NaiveBayesClassifier.train(t)\n",
      "classifier.show_most_informative_features()\n",
      "test_sentence = \"This is the best band I've ever heard\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most Informative Features\n",
        "                    take = True              neg : pos    =      6.3 : 1.0\n",
        "                     got = True              neg : pos    =      6.3 : 1.0\n",
        "                   watch = True              neg : pos    =      4.5 : 1.0\n",
        "                   don't = True              neg : pos    =      4.5 : 1.0\n",
        "                     via = True              neg : pos    =      4.5 : 1.0\n",
        "                    find = True              neg : pos    =      4.5 : 1.0\n",
        "                 someone = True              neg : pos    =      4.5 : 1.0\n",
        "              everything = True              neg : pos    =      4.5 : 1.0\n",
        "                    wish = True              neg : pos    =      4.5 : 1.0\n",
        "                     way = True              neg : pos    =      4.2 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def document_features(document): \n",
      "    document_words = set(document)\n",
      "    features = {}\n",
      "    for word in word_features:\n",
      "        features['contains(%s)' % word] = (word in document_words)\n",
      "    return features\n",
      "\n",
      "\n",
      "def get_features(sentence):\n",
      "    return {word.lower(): (word in feature_selection(sentence.lower())) for word in common_words}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_sentiment(tweet):\n",
      "    features = get_features(tweet)\n",
      "    cls = classifier.classify(features)\n",
      "    x = classifier.prob_classify(features)\n",
      "    #print x.prob('pos')\n",
      "    #print x.prob('neg')\n",
      "    #print x.logprob('pos')\n",
      "    #print x.logprob('neg')\n",
      "    return cls, classifier.prob_classify(features).prob(cls)\n",
      "\n",
      "#print get_sentiment(filteredtweets[filteredtweets['id_str'] ==  454471456127451136]['text'].item())\n",
      "print get_sentiment('go get photo time tomorrow')\n",
      "for pair in train[:10]:\n",
      "   print get_sentiment(pair[0])\n",
      "#print nltk.classify.util.accuracy(classifier, train)\n",
      "#tweets['sentiment'] = tweets['sentiment'].apply(get_features)\n",
      "#tweets['sentiment'].head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('pos', 0.9977046543279084)\n",
        "('neg', 0.71860834527789)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('pos', 0.9963003829067816)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('pos', 0.972662789260423)\n",
        "('neg', 0.6742908202376635)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('pos', 0.9963659049925798)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('pos', 0.9985426315526823)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('pos', 0.9858287366423245)\n",
        "('pos', 0.9952327796499492)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('pos', 0.9726297562865834)\n",
        "('pos', 0.9604351394574968)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print get_features(test_sentence)\n",
      "classifier.classify(get_features(test_sentence))\n",
      "x = classifier.prob_classify(get_features(test_sentence))\n",
      "print x.logprob('pos')\n",
      "print x.logprob('neg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-3.57613068616e-05\n",
        "-15.3000252687\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "filteredtweets['sentiment'] = filteredtweets['sentiment'].apply(get_sentiment)\n",
      "#tweets[tweets['sentiment'] != 'neg'].count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "u'no item named sentiment'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-21-e618c416836e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfilteredtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilteredtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_sentiment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#tweets[tweets['sentiment'] != 'neg'].count()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1633\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1634\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1635\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1640\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1642\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionaility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    981\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2752\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_for_nan_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2754\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2755\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2756\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m_find_block\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   3063\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_find_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3065\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_have\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3066\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m_check_have\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   3070\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_have\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3071\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3072\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no item named %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint_thing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m     def reindex_axis(self, new_axis, indexer=None, method=None, axis=0,\n",
        "\u001b[1;31mKeyError\u001b[0m: u'no item named sentiment'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filteredtweets['sentiment_score'] = filteredtweets['sentiment'].apply(lambda x: x[1])\n",
      "filteredtweets.head(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}